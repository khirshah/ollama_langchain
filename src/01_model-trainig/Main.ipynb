{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils  import * \n",
    "from langchain.llms import Ollama\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading orca-mini from Ollama\n",
    "llm = Ollama(model=\"orca-mini\", temperature=0)\n",
    "\n",
    "# Loading the Embedding Model\n",
    "embed = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and splitting the documents\n",
    "docs = load_pdf_data(file_path=\"./../../data/ml_book.pdf\")\n",
    "documents = split_docs(documents=docs)\n",
    "\n",
    "# creating vectorstore\n",
    "vectorstore = create_embeddings(documents, embed)\n",
    "\n",
    "# converting vectorstore to a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the prompt from the template which we created before\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Creating the chain\n",
    "chain = load_qa_chain(retriever, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'What is random forest?', 'result': ' Random Forest is an ensemble learning technique that involves building multiple decision trees and combining their predictions to improve accuracy. It is a popular method used in machine learning and data mining. Each tree in a Random Forest is grown randomly, with the number of splits (i.e., the number of features used for splitting) chosen randomly from a fixed set of possible values. This randomization helps to reduce overfitting and increase generalization performance. Random Forests have been found to be effective in classification and regression tasks, and are known for their ability to handle high-dimensional data and complex relationships between variables.', 'source_documents': [Document(page_content='9 “Random Decision Forests,” T. Ho (1995).\\n10 The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees.\\n11 There are a few notable exceptions: splitter is absent (forced to \"random\"), presort is absent (forced to\\nFalse), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassi\\nfier with the provided hyperparameters).\\nSampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.\\nRandom Forests\\nAs we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\\ntrained via the bagging method (or sometimes pasting), typically with max_samples\\nset to the size of the training set. Instead of building a BaggingClassifier and pass‐\\ning it a DecisionTreeClassifier, you can instead use the RandomForestClassifier\\nclass, which is more convenient and optimized for Decision Trees10 (similarly, there is', metadata={'source': './../../data/ml_book.pdf', 'file_path': './../../data/ml_book.pdf', 'page': 224, 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'iLovePDF', 'creationDate': \"D:20190507155131+00'00'\", 'modDate': 'D:20231020021301Z', 'trapped': ''}), Document(page_content='12 “Extremely randomized trees,” P. Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nYou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier class. Similarly, the Extra', metadata={'source': './../../data/ml_book.pdf', 'file_path': './../../data/ml_book.pdf', 'page': 225, 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'iLovePDF', 'creationDate': \"D:20190507155131+00'00'\", 'modDate': 'D:20231020021301Z', 'trapped': ''}), Document(page_content='a RandomForestRegressor class for regression tasks). The following code trains a\\nRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\\nall available CPU cores:\\nfrom sklearn.ensemble import RandomForestClassifier\\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\\nrnd_clf.fit(X_train, y_train)\\ny_pred_rf = rnd_clf.predict(X_test)\\nWith a few exceptions, a RandomForestClassifier has all the hyperparameters of a\\nDecisionTreeClassifier (to control how trees are grown), plus all the hyperpara‐\\nmeters of a BaggingClassifier to control the ensemble itself.11\\nThe Random Forest algorithm introduces extra randomness when growing trees;\\ninstead of searching for the very best feature when splitting a node (see Chapter 6), it\\nsearches for the best feature among a random subset of features. This results in a\\ngreater tree diversity, which (once again) trades a higher bias for a lower variance,', metadata={'source': './../../data/ml_book.pdf', 'file_path': './../../data/ml_book.pdf', 'page': 224, 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'iLovePDF', 'creationDate': \"D:20190507155131+00'00'\", 'modDate': 'D:20231020021301Z', 'trapped': ''}), Document(page_content='CHAPTER 7\\nEnsemble Learning and Random Forests\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 7 in the final\\nrelease of the book.\\nSuppose you ask a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you will\\noften get better predictions than with the best individual predictor. A group of pre‐\\ndictors is called an ensemble; thus, this technique is called Ensemble Learning, and an\\nEnsemble Learning algorithm is called an Ensemble method.\\nFor example, you can train a group of Decision Tree classifiers, each on a different', metadata={'source': './../../data/ml_book.pdf', 'file_path': './../../data/ml_book.pdf', 'page': 216, 'total_pages': 510, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'author': '', 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'iLovePDF', 'creationDate': \"D:20190507155131+00'00'\", 'modDate': 'D:20231020021301Z', 'trapped': ''})]}\n",
      " Random Forest is an ensemble learning technique that involves building multiple decision trees and\n",
      "combining their predictions to improve accuracy. It is a popular method used in machine learning and\n",
      "data mining. Each tree in a Random Forest is grown randomly, with the number of splits (i.e., the\n",
      "number of features used for splitting) chosen randomly from a fixed set of possible values. This\n",
      "randomization helps to reduce overfitting and increase generalization performance. Random Forests\n",
      "have been found to be effective in classification and regression tasks, and are known for their\n",
      "ability to handle high-dimensional data and complex relationships between variables.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is random forest?\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " A voting classifier is a type of machine learning algorithm that predicts the class of a given\n",
      "input data point based on the majority vote of multiple classifiers. In other words, it takes the\n",
      "predictions of several different classifiers and predicts the class that gets the most votes. This\n",
      "method is often used to improve the accuracy of machine learning models by using the diversity of\n",
      "different classifiers to make more accurate predictions. The hard voting classifier is a specific\n",
      "type of voting classifier that achieves high accuracy by aggregating the predictions of multiple\n",
      "weak learners.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is Voting Classifier?\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Voting classifiers and Random Forests are two different approaches to creating an ensemble of\n",
      "classifiers.   Voting classifiers use majority vote to predict the class that receives the most\n",
      "votes from the classifiers in the ensemble. This approach is based on the idea that the majority\n",
      "opinion is likely to be correct, and by aggregating the predictions of multiple classifiers, voting\n",
      "can help reduce overfitting and improve the accuracy of the final prediction.   On the other hand,\n",
      "Random Forests are an ensemble learning technique that use decision trees as base classifiers. In\n",
      "this approach, a random subset of the features is used to create a Decision Tree Classifier (DTC) in\n",
      "each iteration of the training process. The final prediction is made by aggregating the predictions\n",
      "of all the DTCs in the forest. Random Forests are known for their ability to handle large datasets\n",
      "and complex decision boundaries, making them a popular choice for many applications.   In summary,\n",
      "voting classifiers use majority vote to predict the class that receives the most votes from the\n",
      "classifiers in the ensemble, while Random Forests use decision trees as base classifiers and make\n",
      "final predictions by aggregating the predictions of all the DTCs in the forest.\n"
     ]
    }
   ],
   "source": [
    "get_response(\"What is the difference between voting classifier and random forest?\", chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
